rmse_elastic_net
library(tidyverse)
hotel_review<-read_csv("Data/hotel-reviews.csv")
library(tidyverse)
hotel_review<-read_csv("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 7/hotel-reviews.csv")
hotel_review<-read_csv("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 1/hotel-reviews.csv")
hotel_review<-sample_n(hotel_review, 5000)
# Convert binary variable, Is_Response to a continuous ratings variable
convert_fn <- function(x) {
sapply(x, function(y) {
if (y == "not happy") {
return(sample(1:2, size = 1, prob = c(0.5, 0.5)))
} else { # y == "happy"
return(sample(3:5, size = 1, prob = c(0.33, 0.33, 0.34)))
}
})
}
hotel_review$ratings <- convert_fn(hotel_review$Is_Response)
head(hotel_review)
library(caret)
library(magrittr)
# Set the seed for reproducibility
set.seed(123)
# Define the proportion of data you want to keep for training
trainIndex <- createDataPartition(hotel_review$ratings, p = 0.70, list = FALSE, times = 1)
# Create the training set
train_set <- hotel_review[trainIndex,]
# Create the testing set
test_set <- hotel_review[-trainIndex,]
library(sentimentr)
t1 <- train_set %>%
mutate(review = get_sentences(Description)) %$%
sentiment_by(review, User_ID)
train_set1 <- train_set %>% left_join(t1,by = "User_ID")
Train <- train_set1 %>% select(User_ID,ratings, word_count, ave_sentiment)
ctrl <- trainControl(method="cv", number=10)
# Create a linear regression model
model <- train(ratings ~ word_count+ ave_sentiment, data=Train, method="lm", trControl=ctrl)
# Print model results
print(model)
summary(model)
t2 <- test_set %>%
mutate(review = get_sentences(Description)) %$%
sentiment_by(review, User_ID)
test_set1 <- test_set %>% left_join(t2,by = "User_ID")
Test <- test_set1 %>% select(User_ID,ratings, word_count, ave_sentiment)
# Generate predictions on test data
predictions <- predict(model, newdata = Test)
# Calculate R-squared and RMSE
rsq_rmse <- postResample(pred = predictions, obs = Test$ratings)
# Print the results
print(rsq_rmse)
library(caret)
library(quanteda)
review_corpus<-corpus(hotel_review, text_field = "Description")
hotel_review_token <- tokens(review_corpus, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE)
#Create DFM
hotel_review_tfidf<-hotel_review_token %>%
tokens_remove(stopwords(source = "smart")) %>%
tokens_wordstem() %>%
tokens_tolower() %>%
dfm() %>%
dfm_trim( min_termfreq = 50, min_docfreq = 10) %>%
dfm_tfidf()
library("quanteda.textmodels")
mylsa <- textmodel_lsa(hotel_review_tfidf, nd = 100)
new.dataset <- data.frame(mylsa$docs)
new.dataset$y.var <- hotel_review$ratings
test_index <- createDataPartition(new.dataset$y.var, p = .2, list = F)
Test_LSA<-new.dataset[test_index,]
Train_LSA<-new.dataset[-test_index,]
ctrl <- trainControl(method="cv", number=10)
#Training data
# Create a linear regression model
model <- train(y.var ~ ., data=Train_LSA, method="lm", trControl=ctrl)
summary(model)
# Generate predictions on test data
predictions1 <- predict(model, newdata = Test_LSA)
# Calculate R-squared and RMSE
rsq_rmse1 <- postResample(pred = predictions1, obs = Test_LSA$y.var)
# Print the results
print(rsq_rmse1)
hotel_review_tfidf1 <- data.frame(y.var=hotel_review$ratings,hotel_review_tfidf )
hotel_review_tfidf1 <- hotel_review_tfidf1 %>% select(-doc_id)
test_index <- createDataPartition(hotel_review_tfidf1$y.var, p = .2, list = F)
hotel_review_tfidf1 <- data.frame(y.var=hotel_review$ratings,hotel_review_tfidf )
hotel_review_tfidf1 <- hotel_review_tfidf1 %>% select(-doc_id)
hotel_review_tfidf1 <- data.frame(y.var=hotel_review$ratings,hotel_review_tfidf )
hotel_review_tfidf1 <- hotel_review_tfidf1 %>% select(-doc_id)
# Convert the dfm object to a data.frame
hotel_review_tfidf_df <- convert(hotel_review_tfidf, to = "data.frame")
# Combine the ratings with the tfidf data frame
hotel_review_tfidf1 <- data.frame(y.var = hotel_review$ratings, hotel_review_tfidf_df)
test_index <- createDataPartition(hotel_review_tfidf1$y.var, p = .2, list = F)
test_text <- hotel_review_tfidf1[test_index, ]
train_text <- hotel_review_tfidf1[-test_index, ]
## Set up repeated k-fold cross-validation
train_control <- trainControl(method = "cv", number = 2)
# Fit the SVR model
set.seed(12345)
SVM_Linear <- train(
y.var ~., data = train_text, method = "svmLinear",
trControl = train_control,
preProcess = c("center","scale"),
tuneLength = 7
)
# Predict on the test set
predictions <- predict(SVM_Linear, newdata = test_text)
new_data <- dfm_match(df_test, features = featnames(df_training))
# Predict with the final Ridge model
predictions_ridge <- predict(final_ridge, newx = new_data)
# Predict with the final Lasso model
predictions_lasso <- predict(final_lasso, newx = new_data)
# Predict with the final Elastic Net model
predictions_elastic_net <- predict(final_elastic_net, newx = new_data)
library(caret)
# Calculate R-squared for the Ridge model
r_squared_ridge <- postResample(pred = predictions_ridge, obs = hotel_test$ratings)[2]
r_squared_ridge
# Calculate RMSE for the Ridge model
rmse_ridge <- postResample(pred = predictions_ridge, obs = hotel_test$ratings)[1]
rmse_ridge
# Calculate R-squared for the Lasso model
r_squared_lasso <- postResample(pred = predictions_lasso, obs = hotel_test$ratings)[2]
r_squared_lasso
# Calculate RMSE for the Lasso model
rmse_lasso <- postResample(pred = predictions_lasso, obs = hotel_test$ratings)[1]
rmse_lasso
# Calculate R-squared for the Elastic Net model
r_squared_elastic_net <- postResample(pred = predictions_elastic_net, obs = hotel_test$ratings)[2]
r_squared_elastic_net
# Calculate RMSE for the Elastic Net model
rmse_elastic_net <- postResample(pred = predictions_elastic_net, obs = hotel_test$ratings)[1]
rmse_elastic_net
library(tidyverse)
library(bigrquery)
install.packages("gganimate")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("sf")
install.packages("gganimate")
install.packages("ggplot2")
install.packages("dplyr")
library(gganimate)
library(dplyr)
library(ggplot2)
library(sf)
project_id <- 'is-6850-090-advanced-sql'
setwd(getwd())
install.packages("ggplot2")
install.packages("dplyr")
library(tidyverse)
library(bigrquery)
install.packages("gganimate")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("sf")
install.packages("dplyr")
install.packages("ggplot2")
library(gganimate)
library(dplyr)
library(ggplot2)
library(sf)
project_id <- 'is-6850-090-advanced-sql'
setwd(getwd())
install.packages("ggplot2")
library(tidyverse)
library(bigrquery)
library(gganimate)
library(dplyr)
library(ggplot2)
library(sf)
project_id <- 'is-6850-090-advanced-sql'
setwd(getwd())
get_query <- function(x){
con <- file(x, "r")
contents <- readChar(con, file.info(x)$size)
close(con)
return(contents)
}
sql4 <- get_query('final_project3.sql')
qq4 <- bq_project_query(project_id, sql4)
sql4 <- get_query('final_project3.sql')
qq4 <- bq_project_query(project_id, sql4)
dta4 <- bq_table_download(qq4)
dta4 <- dta4 %>%
mutate(hour = factor(hour, levels = 0:23))
# If you have a shapefile of the stations, load it
# stations_shapefile <- st_read("path_to_shapefile.shp")
# Create the animated plot
p <- ggplot(dta4, aes(x = lon, y = lat, size = bike_count, color = bike_count)) +
geom_point(alpha = 0.7) +
scale_size(range = c(3, 10), name = "Bike Count") +
scale_color_viridis_c() +
labs(title = "Number of Bikes by Hour: {closest_state}", x = "Longitude", y = "Latitude") +
theme_minimal() +
transition_states(hour, transition_length = 2, state_length = 1) +
ease_aes('linear')
# Animate the plot
animate(p, nframes = 24, fps = 2, width = 800, height = 600, renderer = gifski_renderer("animated_bikes.gif"))
# Animate the plot
animate(p, nframes = 24, fps = 2, width = 800, height = 600, renderer = gifski_renderer("animated_bikes.gif"))
setwd(getwd())
# Confirm the working directory
print(getwd())
setwd("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA")
# Confirm the working directory
print(getwd())
# Load necessary libraries
library(tidyverse)
library(e1071)
# Confirm the working directory
print(getwd())
# Load necessary libraries
library(tidyverse)
library(e1071)
library(caret)
library(quanteda)
library(irlba)
# Load the data
data <- read.csv('sample_airbnb.csv', encoding = 'latin1')
# Convert the relevant columns to factors
data$host_is_superhost <- as.factor(data$host_is_superhost)
# Partition the data
set.seed(12345)
data_partition <- createDataPartition(data$host_is_superhost, times = 1, p = 0.7, list = FALSE)
train_data <- data[data_partition, ]
test_data <- data[-data_partition, ]
# Create tokens from the comments column
train_tokens <- tokens(train_data$comments, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords(source = "smart")) %>%
tokens_wordstem()
# Create a document-feature matrix (DFM)
train_dfm <- dfm(train_tokens) %>%
dfm_trim(min_termfreq = 10, min_docfreq = 2) %>%
dfm_tfidf()
# Perform SVD to reduce dimensionality
set.seed(12345)
train_lsa <- irlba(t(train_dfm), nv = 100, maxit = 600)
train_svd <- data.frame(Label = train_data$host_is_superhost, ReviewLength = nchar(train_data$comments), train_lsa$v)
# Check for and remove rows with missing values
train_svd <- na.omit(train_svd)
# Create stratified folds for cross-validation
set.seed(48743)
cv_folds <- createMultiFolds(train_svd$Label, k = 10, times = 2)
cv_control <- trainControl(method = "repeatedcv", number = 10, repeats = 2, index = cv_folds)
# Train the SVM model
set.seed(12345)
svm_model <- train(Label ~ ., data = train_svd, method = "svmPoly",
preProcess = c("center", "scale"),
trControl = cv_control, tuneLength = 7)
# Print the model summary
print(svm_model)
# Make predictions on the training data
train_preds <- predict(svm_model, train_svd)
confusionMatrix(train_preds, train_svd$Label)
# Tokenize the test data
test_tokens <- tokens(test_data$comments, what = "word", remove_numbers = TRUE, remove_punct = TRUE, remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(stopwords(source = "smart")) %>%
tokens_wordstem()
# Create a document-feature matrix (DFM) for the test data
test_dfm <- dfm(test_tokens) %>%
dfm_trim(min_termfreq = 10, min_docfreq = 2) %>%
dfm_tfidf()
# Match the features of the test DFM with the training DFM
test_dfm <- dfm_match(test_dfm, features = featnames(train_dfm))
# Create the test SVD matrix
sigma_inverse <- 1 / train_lsa$d
u_transpose <- t(train_lsa$u)
test_svd <- t(sigma_inverse * u_transpose %*% t(test_dfm))
test_svd <- as.matrix(test_svd)
# Create a data frame for the test data
test_svd <- data.frame(Label = test_data$host_is_superhost, ReviewLength = nchar(test_data$comments), test_svd)
# Check for and remove rows with missing values in test data
test_svd <- na.omit(test_svd)
# Make predictions on the test data
test_preds <- predict(svm_model, test_svd)
confusionMatrix(test_preds, test_svd$Label)
