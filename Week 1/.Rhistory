#MLP model
cv_function(df, target, folds, seedVal, ksvm, tree_metrics)
#MLP model
cv_function(df, target, folds, seedVal, prediction_method = ksvm, tree_metrics)
target <- 15
folds <- 5
seedVal <- 100
#MLP model
cv_function(df, target, folds, seedVal, ksvm, tree_metrics)
#MLP model
cv_function(df, target, folds, seedVal, ksvm, metrics)
train_target <- df[-x,target]
cv_function <- function(df, target, nFolds, seedVal, prediction_method, metrics_list)
{
# create folds
set.seed(seedVal)
folds = createFolds(df[,target],nFolds)
# perform cross validation
cv_results <- lapply(folds, function(x)
{
test_target <- df[x,target]
test_input  <- df[x,-target]
train_target <- df[-x,target]
train_input <- df[-x,-target]
prediction_model <- ksvm(train_target~.,train_input)
pred<- predict(prediction_model,test_input)
return(mmetric(test_target,pred,metrics_list))
})
# generate means and sds and show cv results, means and sds using kable
cv_results_m <- as.matrix(as.data.frame(cv_results))
cv_mean<- as.matrix(rowMeans(cv_results_m))
cv_sd <- as.matrix(rowSds(cv_results_m))
colnames(cv_mean) <- "Mean"
colnames(cv_sd) <- "Sd"
cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
kable(t(cv_all),digits=2)
}
target <- 15
folds <- 5
seedVal <- 100
#MLP model
cv_function(df, target, folds, seedVal, ksvm, tree_metrics)
tree
summary(tree)
View(df)
View(df)
#check for na
(df %>% summarize(across(everything(), ~ sum(. == '?')))
#check for na
(df %>% summarize(across(everything(), ~ sum(. == '?'))))
df %>% summarise_all(~ sum(. == '?'))
(df %>% summarise_all(~ sum(. == '?')))
(df %>% summarise(across(everything(), ~ sum(. == '?'))))
unique(df$education)
unique(df$occupation)
(df %>% summarise(across(everything(), ~ sum(. == '?                '))))
#KSVM model
cv_function(df, target, folds, seedVal, ksvm, tree_metrics)
#KSVM model
cv_function(df, target, folds, seedVal, C5.0, tree_metrics)
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
target <- 15
folds <- 5
seedVal <- 100
#KSVM model
cv_function(df, target, folds, seedVal, C5.0, tree_metrics)
knitr::opts_chunk$set(echo = TRUE, message = F, warning = F)
library(dplyr)
library(C50)
library(e1071)
library(caret)
library(rminer)
library(rmarkdown)
library(psych)
library(rpart)
library(rpart.plot)
library(rJava)
library(RWeka)
library(matrixStats)
library(knitr)
library(tidyverse)
library(psych)
library(party)
library(kernlab)
df <- read.csv('census.csv', stringsAsFactors = TRUE)
str(df)
head(df)
summary(df)
df <- df %>% mutate(education.num = factor(education.num))
summary(df)
#identify the Other levels from summary
unique(df$workclass)
unique(df$education)
unique(df$occupation)
#check for na
(df %>% summarize(across(everything(), ~ sum(is.na(.)))))
#correlation
df %>% select(where(is.numeric)) %>% pairs.panels()
#capital gain distribution
hist((df$capital.gain), xlab = 'capital.gain', ylab = 'n', main = 'Capital Gain')
hist(log(df$capital.gain), xlab = 'capital.gain', ylab = 'n', main = 'Capital Gain')
df_log <- df %>% mutate(capital.gain = log(capital.gain), capital.loss = log(capital.loss))
df %>% ggplot() + geom_boxplot(aes(x = y, y = hours.per.week)) + ggtitle("y - hour per week")
df %>% ggplot() + geom_boxplot(aes(x = y, y = age)) + ggtitle("y - age")
barplot(table(df$occupation, df$y),xlim = c(0, 5),  ylim = c(0,25000), legend = TRUE, main = 'y - occupation', xlab = "y", ylab = "Occupation")
barplot(table(df$education, df$y),xlim = c(0, 5),  ylim = c(0,25000), legend = TRUE, main = 'y against education level', xlab = "y - target", ylab = "Education Level")
#marital status against y proportion
prop.table(table(df$marital.status, df$y))*100
#relationship against y proportion
prop.table(table(df$relationship, df$y))*100
#distribution of income against sex
df %>% group_by(y, sex) %>% summarise(Count= n()) %>% ggplot() + geom_col(aes(x=sex, y=Count, fill = y)) + ggtitle("Sex - y")
#Simple distribution of income classification
plot(df$y,xlim = c(0, 5),  ylim = c(0,25000), main = "Distribution of y")
#percentage distribution of income - y in data set, the accuracy of the majority classifier will be 75.9%
prop.table(table(df$y))*100
#setting seed
set.seed(100)
#finding the target variable
names(df)
#partition set
inTrain <- createDataPartition(y=df$y, p = 0.70, list=FALSE)
train_target <- df[inTrain,15]
test_target <- df[-inTrain,15]
train_input <- df[inTrain,-15]
test_input <- df[-inTrain,-15]
#Proprtion of y in train and test
prop.table(table(train_target))*100
prop.table(table(test_target))*100
cv_function <- function(df, target, nFolds, seedVal, prediction_method, metrics_list)
{
# create folds
set.seed(seedVal)
folds = createFolds(df[,target],nFolds)
# perform cross validation
cv_results <- lapply(folds, function(x)
{
test_target <- df[x,target]
test_input  <- df[x,-target]
train_target <- df[-x,target]
train_input <- df[-x,-target]
prediction_model <- ksvm(train_target~.,train_input)
pred<- predict(prediction_model,test_input)
return(mmetric(test_target,pred,metrics_list))
})
# generate means and sds and show cv results, means and sds using kable
cv_results_m <- as.matrix(as.data.frame(cv_results))
cv_mean<- as.matrix(rowMeans(cv_results_m))
cv_sd <- as.matrix(rowSds(cv_results_m))
colnames(cv_mean) <- "Mean"
colnames(cv_sd) <- "Sd"
cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
kable(t(cv_all),digits=2)
}
#metrics list - "MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"
metrics <- c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2")
tree_metrics <- c("ACC","F1","PRECISION","TPR")
#Tried Combinations
#CF=0.9 treesize=472 acc=85.99
#cf=0.5 treesize=190 acc=86.47
#cf=0.15 treesize = 38 acc=86.5
#cf=0.08 treesize = 20 acc=85.5
tree <- C5.0(train_input, train_target, control = C5.0Control(CF=0.01, earlyStopping = FALSE, noGlobalPruning = FALSE))
tree_test <- predict(tree, test_input)
tree$size
summary(tree)
mmetric(test_target, tree_test, metric=tree_metrics)
nb_model <- naiveBayes(train_target ~ ., data = train_input)
nb_prediction_test <- predict(nb_model, test_input)
mmetric(test_target, nb_prediction_test, metric=tree_metrics)
rpart_model <- rpart(train_target~., data = train_input)
rpart_pred_test <- predict(rpart_model, test_input)
mmetric(test_target, rpart_pred_test, tree_metrics)
ibk <- IBk(train_target ~ .,data = train_input, control = Weka_control(K=40,X=TRUE, I=TRUE))
ibk_pred_test <- predict(ibk, test_input)
mmetric(test_target, ibk_pred_test, tree_metrics)
ksvm <- ksvm(train_target ~ .,data = train_input, kernel = "laplacedot", C=5)
ksvm_pred <- predict(ksvm, test_input)
mmetric(test_target, ksvm_pred, tree_metrics)
target <- 15
folds <- 5
seedVal <- 100
#KSVM model
cv_function(df, target, folds, seedVal, C5.0, tree_metrics)
gc()
#commenting for faster knitting
#naiveBayes model
#cv_function(df, target, folds, seedVal, naiveBayes, tree_metrics)
#rpart model
#cv_function(df, target, folds, seedVal, rpart, tree_metrics)
#IBk model
#cv_function(df, target, folds, seedVal, IBk, tree_metrics)
#ksvm model
#commenting for faster knitting
#cv_function(df, target, folds, seedVal, ksvm, tree_metrics)
data <- data.frame(review = reviews, rating = ratings)
my_tokens <- my_corpus %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords()) %>%
tokens_wordstem()
library(tidyverse)
library(tidytext)
reviews <- c("This product is amazing! I highly recommend it.",
"I'm quite disappointed with the quality. It broke after a few uses.",
"The customer service was excellent. They promptly resolved my issue.",
"The price is reasonable for the features it offers.",
"I had a terrible experience with this product. It doesn't work as advertised.")
ratings <- c(5, 2, 4, 4, 1)
data <- data.frame(review = reviews, rating = ratings)
data %>% mutate(review = tolower(review))
toLower(data$review)
data %>% unnest_tokens(review, tolower(review))
library(tidyverse)
library(tidytext)
install.packages("tidytext")
toLower(data$review)
data %>% unnest_tokens(review, tolower(review))
toLower(data)
data %>% mutate(word_count = nchar(review))
word_count(data$review)
data %>% unnest_tokens(word, review) %>% count(review)
data %>% mutate(word_count = str_count(review, "\\w+"))
str_remove_all(data$review, c("product", "price"))
data
str_remove_all(data$review, c("product", "price"))
clear
str_remove_all(data$review, c("product", "price"))
data %>% unnest_tokens(review, review) %>% filter(!review %in% c("product", "price"))
data %>% mutate(review = str_remove_all(review, c("product", "price")))
data %>% unnest_tokens(review, str_remove_all(review, c("product", "price")))
str_remove_all(data$review, c("product", "price"))
data %>% mutate(review = str_remove_all(review, c("product", "price")))
data %>% mutate(review = str_remove_all(review, c("product", "price")))
str_remove_all(review, c("product", "price"))
str_remove_all(data$review, c("product", "price"))
data %>%
mutate(review = ifelse(str_detect(review, "product|price"),
str_remove_all(review, c("product", "price")),
review))
library(quanteda)
install.packages("quanteda")
install.packages("quanteda")
library(quanteda)
install.packages("quanteda")
library(quanteda)
textstat.collocations(data$review)
quanteda.textstats::textstat_collocations(data$review)
library(quanteda)
library(quanteda)
quanteda.textstats::textstat_collocations(data$review)
install.packages("quanteda.textstats")
quanteda.textstats::textstat_collocations(data$review)
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
textplot_wordcloud(data_df)
install.packages("quanteda.textplots")
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda.textstats)
review_corpus <- corpus(data$review)
corpus_sentences <- corpus_reshape(review_corpus, to = "sentences")
corpus_sentences
review_corpus
corpus_tokens <- corpus_sentences %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE) %>%
tokens_tolower()
review_collocations <- textstat_collocations(corpus_tokens, min_count = 2)
corpus_tokens
review_collocations
corpus_tokens <- tokens_compound(corpus_tokens, review_collocations)
corpus_tokens
minimumFrequency <- 2
DTM <- corpus_tokens %>%
tokens_remove("") %>%
dfm()
rowSums(DTM)
boolDTM <- corpus_tokens %>%
tokens_remove("") %>%
dfm() %>%
dfm_trim(min_docfreq = minimumFrequency) %>%
dfm_weight("boolean")
rowSums(boolDTM)
t(boolDTM)
boolDTM
coocCounts <- t(boolDTM) %*% boolDTM
as.matrix(coocCounts)
data %>% unnest_tokens(review, review) %>% filter(!review %in% c("product", "price"))
install.packages("tidytext")
library(tidytext)
data %>% unnest_tokens(review, review) %>% filter(!review %in% c("product", "price"))
data <- data.frame(review = reviews, rating = ratings)
data %>% mutate(review = tolower(review))
data %>% mutate(word_count = str_count(review, "\\w+"))
data %>% unnest_tokens(review, review) %>% filter(!review %in% c("product", "price"))
data <- data.frame(review = reviews, rating = ratings)
data %>% mutate(review = tolower(review))
library(tidyverse)
data <- data.frame(review = reviews, rating = ratings)
data %>% mutate(review = tolower(review))
data %>% mutate(word_count = str_count(review, "\\w+"))
data %>% unnest_tokens(review, review) %>% filter(!review %in% c("product", "price"))
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE)
textplot_wordcloud(data_df)
library(quanteda)
library(quanteda.textplots)# for word clouds
data_df <- quanteda::tokens(data$review,
what = "word",
remove_numbers = TRUE,
remove_punct = TRUE,
remove_symbols = TRUE) %>% dfm()
textplot_wordcloud(data_df)
library(quanteda)
library(readtext)# https://readtext.quanteda.io/articles/readtext_vignette.html
library(tidyverse
library(quanteda)
library(quanteda)
library(readtext)
library(tidyverse)
data
data <- read.csv("deceptive-opinion-1.csv")
get(wd)
getwd()
setwd("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 1/")
getwd()
data <- read.csv("deceptive-opinion-1.csv")
data
library(dplyr)
# Randomly sample 500 rows from the dataset
sample_data <- data %>% sample_n(500)
View(sample_data)
sample_data %>% select(deceptive = 'truthful')
sample_data <- sample_data %>%
mutate(review_length = str_length(review))
View(sample_data)
View(sample_data)
sample_data <- sample_data %>%
mutate(review_length = str_length(review))
View(sample_data)
sample_data %>%
filter(truthful == "truthful") %>%
group_by(positive) %>%
summarize(average_length = mean(review_length),
sd_length = sd(review_length),
n = n())
sample_data %>%
filter(deceptive == "truthful") %>%
group_by(positive) %>%
summarize(average_length = mean(review_length),
sd_length = sd(review_length),
n = n())
sample_data %>%
filter(deceptive == "truthful") %>%
group_by(polarity) %>%
summarize(average_length = mean(str_length(text)),
n = n())
sample_data %>%
filter(deceptive == "deceptive") %>%
group_by(polarity) %>%
summarize(average_length = mean(str_length(text)),
n = n())
sample_corpus <- corpus(sample_data, text_field = text)
sample_corpus <- corpus(sample_data, text_field = "text")
head(sample_corpus, 3)
sample_dfm <- sample_corpus %>%
tokens(remove_punct = TRUE, remove_numbers = TRUE,
remove_symbols = TRUE) %>%
tokens_tolower() %>%
tokens_remove(pattern = stopwords()) %>%
tokens_wordstem() %>%
dfm()
dfm
sample_dfm
dim(sample_dfm)
sparsity(sample_dfm)
library(ggplot2)
library(tidytext)
data$text_tidy <- preprocess(data$text, type = c("sentence", "word")) %>%
mutate(word = tolower(word)) %>%
filter(!word %in% stopwords("english"))
sample_dfm
# Create bar plot
ggplot(sample_dfm, aes(x = features, y = n)) +
geom_bar(stat = "identity") +
labs(title = "Most Frequent Words", x = "Word", y = "Frequency") +
theme_bw()
# Preprocess text for tidytext
tidy_df <- data %>%
mutate(text = tolower(text)) %>%
mutate(text = str_replace_all(text, "[[:punct:]]", "")) %>%
mutate(text = str_replace_all(text, "\\d+", ""))
# Tokenize and count word frequencies
tokens <- tidy_df %>%
unnest_tokens(word, text)
View(tokens)
word_counts <- tokens %>%
count(word, sort = TRUE)
View(word_counts)
# Plot the most frequent words
top_words <- word_counts %>% top_n(20, n)
ggplot(top_words, aes(x = reorder(word, n), y = n)) +
geom_bar(stat = "identity") +
coord_flip() +
labs(title = "Most Frequent Words", x = "Words", y = "Frequency")
# Tokenize and count word frequencies
tokens <- tidy_df %>%
unnest_tokens(word, text) %>%
tokens_remove(pattern = stopwords()) %>%
word_counts <- tokens %>%
count(word, sort = TRUE)
# Calculate the number of tokens per document
tokens_per_document <- rowSums(sample_dfm)
# View summary statistics
summary(tokens_per_document)
tokens_df <- data.frame(tokens = tokens_per_document)
# Plot the histogram
ggplot(tokens_df, aes(x = tokens)) +
geom_histogram(binwidth = 10, fill = "blue", color = "black", alpha = 0.7) +
labs(title = "Histogram of Tokens Per Document",
x = "Number of Tokens",
y = "Frequency") +
theme_minimal()
textplot_wordcloud(sample_dfm)
textplot_wordcloud(sample_dfm)
sample_dfm
head(sample_corpus, 3)
text %>% tokens(remove_punct = TRUE, remove_numbers = TRUE,
remove_symbols = TRUE)
corpus(text) %>% tokens(remove_punct = TRUE, remove_numbers = TRUE,
remove_symbols = TRUE)
text <- c("It was clean, good service and suite style rooms; however, it’s a little older than some Residence Marriotts’s I’ve stayed at.")
corpus(text) %>% tokens(remove_punct = TRUE, remove_numbers = TRUE,
remove_symbols = TRUE)
corpus(text) %>% tokens()
