test_input <- df[-inTrain,-15]
#Proprtion of y in train and test
prop.table(table(train_target))*100
prop.table(table(test_target))*100
cv_function <- function(df, target, nFolds, seedVal, prediction_method, metrics_list)
{
# create folds
set.seed(seedVal)
folds = createFolds(df[,target],nFolds)
# perform cross validation
cv_results <- lapply(folds, function(x)
{
test_target <- df[x,target]
test_input  <- df[x,-target]
train_target <- df[-x,target]
train_input <- df[-x,-target]
prediction_model <- ksvm(train_target~.,train_input)
pred<- predict(prediction_model,test_input)
return(mmetric(test_target,pred,metrics_list))
})
# generate means and sds and show cv results, means and sds using kable
cv_results_m <- as.matrix(as.data.frame(cv_results))
cv_mean<- as.matrix(rowMeans(cv_results_m))
cv_sd <- as.matrix(rowSds(cv_results_m))
colnames(cv_mean) <- "Mean"
colnames(cv_sd) <- "Sd"
cv_all <- cbind(cv_results_m, cv_mean, cv_sd)
kable(t(cv_all),digits=2)
}
#metrics list - "MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2"
metrics <- c("MAE","RMSE","MAPE","RMSPE","RAE", "RRSE", "COR", "R2")
tree_metrics <- c("ACC","F1","PRECISION","TPR")
#Tried Combinations
#CF=0.9 treesize=472 acc=85.99
#cf=0.5 treesize=190 acc=86.47
#cf=0.15 treesize = 38 acc=86.5
#cf=0.08 treesize = 20 acc=85.5
tree <- C5.0(train_input, train_target, control = C5.0Control(CF=0.01, earlyStopping = FALSE, noGlobalPruning = FALSE))
tree_test <- predict(tree, test_input)
tree$size
summary(tree)
mmetric(test_target, tree_test, metric=tree_metrics)
nb_model <- naiveBayes(train_target ~ ., data = train_input)
nb_prediction_test <- predict(nb_model, test_input)
mmetric(test_target, nb_prediction_test, metric=tree_metrics)
rpart_model <- rpart(train_target~., data = train_input)
rpart_pred_test <- predict(rpart_model, test_input)
mmetric(test_target, rpart_pred_test, tree_metrics)
ibk <- IBk(train_target ~ .,data = train_input, control = Weka_control(K=40,X=TRUE, I=TRUE))
ibk_pred_test <- predict(ibk, test_input)
mmetric(test_target, ibk_pred_test, tree_metrics)
ksvm <- ksvm(train_target ~ .,data = train_input, kernel = "laplacedot", C=5)
ksvm_pred <- predict(ksvm, test_input)
mmetric(test_target, ksvm_pred, tree_metrics)
target <- 15
folds <- 5
seedVal <- 100
#KSVM model
cv_function(df, target, folds, seedVal, C5.0, tree_metrics)
gc()
#commenting for faster knitting
#naiveBayes model
#cv_function(df, target, folds, seedVal, naiveBayes, tree_metrics)
#rpart model
#cv_function(df, target, folds, seedVal, rpart, tree_metrics)
#IBk model
#cv_function(df, target, folds, seedVal, IBk, tree_metrics)
#ksvm model
#commenting for faster knitting
#cv_function(df, target, folds, seedVal, ksvm, tree_metrics)
# Loading the necessary libraries
library(rvest)
library(polite)
library(dplyr)
install.packages("rvest")
install.packages("polite")
# Loading the necessary libraries
library(rvest)
library(polite)
library(dplyr)
# Setting the URL of the page to be scraped
url <- "http://books.toscrape.com/"
# Reading the HTML content of the page
bow(url)
webpage <- read_html(url)
# Extracting a1
a1 <- webpage %>%
html_nodes(".product_pod h3 a") %>%
html_attr("title")
# Extracting b1
b1 <- webpage %>%
html_nodes(".product_pod .price_color") %>%
html_text()
# Creating a data frame
book_data <- data.frame(Name = a1, Price = b1)
a1
webpage %>%
html_nodes(".product_pod h3 a")
b1
b1 <- stringr::str_remove(b1, "£") %>% as.numeric()
b1
# Extracting b1
b1 <- webpage %>%
html_nodes(".product_pod .price_color") %>%
html_text()
b1 <- stringr::str_replace(b1, "£") %>% as.numeric()
library(ralger)
install.packages("ralger")
install.packages("stringr")
library(ralger)
library(stringr)
# Setting the URL of the page to be scraped
my_link <- "https://www.boxofficemojo.com/chart/top_lifetime_gross/?ref_=bo_lnav_hm_shrt"
bow(my_link)
# Defining the nodes to be scraped
my_nodes <- c(
".mojo-field-type-title", # The title
".mojo-field-type-money", # Lifetime Gross
".mojo-field-type-year" # Year
)
# Defining the names of the columns in the output data frame
names <- c("title", "Lifetime_Gross", "Year") # respect the nodes order
# Scraping the data
scraped_data <- tidy_scrap(link = my_link, nodes = my_nodes, colnames = names)
scraped_data
scraped_data <- scraped_data[-1,]
scraped_data
scraped_data1 <- scrap(link = my_link, nodes = my_nodes, colnames = names)
scraped_data
scraped_data$Lifetime_Gross <- str_remove_all(scraped_data$Lifetime_Gross,"[\\$,]")
scraped_data
# Scraping the data
scraped_data <- tidy_scrap(link = my_link, nodes = my_nodes, colnames = names)
scraped_data <- scraped_data[-1,]
#what happens question
scraped_data$Lifetime_Gross <- str_remove_all(scraped_data$Lifetime_Gross,"[\\$]")
scraped_data
scraped_data$Lifetime_Gross <- str_remove_all(scraped_data$Lifetime_Gross,"[\\$,]")
install.packages("httr")
install.packages("jsonlite")
# Load required libraries
library(httr)
library(jsonlite)
# Define the API key
api_key <- "1d34dcca8a9b4d5a8e435de8af933314"
# Define the API endpoint
api_endpoint <- "https://newsapi.org/v2/everything"
# Define the query parameters
query_params <- list(
q = "finance",  # search for articles with 'finance' in them
language = "en",  # get articles in English
sortBy = "publishedAt",  # sort by publication date
apiKey = api_key  # your API key
)
# Make the GET request
response <- GET(url = api_endpoint, query = query_params)
# Parse the JSON response
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
print(data$articles)
query_params <- list(
q = "economics",
language = "en",
sortBy = "publishedAt",
apiKey = api_key
)
response <- GET(url = api_endpoint, query = query_params)
# Parse the JSON response
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
data
query_params <- list(
query = "economics",
language = "en",
sortBy = "publishedAt",
apiKey = api_key
)
response <- GET(url = api_endpoint, query = query_params)
# Parse the JSON response
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
data
data
# Define the query parameters
query_params <- list(
q = "finance",  # search for articles with 'finance' in them
language = "en",  # get articles in English
sortBy = "publishedAt",  # sort by publication date
apiKey = api_key  # your API key
)
# Make the GET request
response <- GET(url = api_endpoint, query = query_params)
# Parse the JSON response
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
data_df <- data.frame(data[["articles"]])
data_df
View(data_df)
length(data$articles)
nrow(data$articles)
length(data)
ncol(data$articles)
nrow(data$articles)
query_params <- list(
q = "economics" AND "finance",
query_params <- list(
q = "economics" + q="finance",
query_params <- list(
q = "economics" and  q="finance",
query_params <- list(
q = "economics & finance",
language = "en",
sortBy = "publishedAt",
apiKey = api_key
)
response <- GET(url = api_endpoint, query = query_params)
# Parse the JSON response
data <- fromJSON(content(response, "text", encoding = "UTF-8"))
data_df <- data.frame(data[["articles"]])
View(data_df)
# Loading the necessary libraries
library(rvest)
# Setting the URL of the page to be scraped
url <- "http://books.toscrape.com/"
# Reading the HTML content of the page
bow(url)
webpage <- read_html(url)
webpage
install.packages("sentimentr")
install.packages("broom")
# Load up the libraries and dataset
library(tidyverse)
library(sentimentr)
library(caret)
library(quanteda)
library(broom)
getwd()
setwd("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 3/")
setwd("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 3/")
# Load up the .CSV data and explore in RStudio.
hotel_raw <- read_csv("hotel-reviews.csv")
set.seed(1234)# we set seed to replicate our results.
hotel_raw<-hotel_raw[sample(nrow(hotel_raw), 5000), ]# take a small sample
corp_hotel <- corpus(hotel_raw, text_field = "Description")# Create corpus
sample<-corpus_sample(corp_hotel, size = 20)# you can sample a corpus too!
library(quanteda)
test.lexicon <- dictionary(list(positive.terms = c("happy", "joy", "light"),
negative.terms = c("sad", "angry", "darkness")))
View(test.lexicon)
testtext<-c("I am happy and confident that the paper will be accepted",
"Of course, no one can be 100% sure but I am hopeful",
"In case, it is rejected, I will be sad and angry, we will submit it to another journal")
dfm_sentiment1 <- testtext %>% tokens() %>% dfm() %>% dfm_lookup(test.lexicon)
dfm_sentiment1
positive_words_bing <- scan("positive-words.txt", what = "char", sep = "\n", skip = 35, quiet = T)
negative_words_bing <- scan("negative-words.txt", what = "char", sep = "\n", skip = 35, quiet = T)
sentiment_bing <- dictionary(list(positive = positive_words_bing, negative = negative_words_bing))
dfm_sentiment <- corp_hotel %>% tokens() %>% dfm %>% dfm_lookup(sentiment_bing)
dfm_sentiment
dfm_sentiment_df<-convert(dfm_sentiment, to ='data.frame')
dfm_sentiment_df$net<-(dfm_sentiment_df$positive)-(dfm_sentiment_df$negative)
summary(dfm_sentiment_df)# document level summary
library("quanteda.dictionaries")
output_mfd <- quanteda.dictionaries::liwcalike(corp_hotel,
dictionary = data_dictionary_MFD)
install.packages("quanteda.dictionaries")
library("quanteda.dictionaries")
install.packages("quanteda.dictionaries")
output_mfd <- quanteda.dictionaries::liwcalike(corp_hotel,
dictionary = data_dictionary_MFD)
head(output_mfd)
install.packages("remotes")
# install.packages("remotes")
remotes::install_github("kbenoit/quanteda.dictionaries")
head(output_mfd)
output_mfd <- quanteda.dictionaries::liwcalike(corp_hotel,
dictionary = data_dictionary_MFD)
library("quanteda.dictionaries")
# install.packages("remotes")
remotes::install_github("kbenoit/quanteda.dictionaries")
dfm_sentiment_prop <- dfm_weight(dfm_sentiment, scheme = "prop")
dfm_sentiment_prop
sentiment <- convert(dfm_sentiment_prop, "data.frame") %>%
gather(positive, negative, key = "Polarity", value = "Share") %>%
mutate(document = as_factor(doc_id)) %>%
rename(Review = document)
ggplot(sentiment, aes(Review, Share, fill = Polarity, group = Polarity)) +
geom_bar(stat='identity', position = position_dodge(), size = 1) +
scale_fill_brewer(palette = "Set1") +
theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust = 1)) +
ggtitle("Sentiment scores in Hotel Reviews (relative)")
mytext<-"I am happy and confident that the paper will be accepted.
Of course, no one can be 100% sure but I am hopeful. In case, it is rejected,
I will be sad and angry, but we will submit it to another journal."
mytext <- get_sentences(mytext)
sentiment(mytext)
dfm_sentiment_prop
dfm_sentiment
sentiment <- convert(dfm_sentiment_prop, "data.frame") %>%
gather(positive, negative, key = "Polarity", value = "Share") %>%
mutate(document = as_factor(doc_id)) %>%
rename(Review = document)
sentiment
mytext
sentiment(mytext)
library(quanteda)
product.lexicon <- dictionary(list(satisfied.terms = c("satisfied", "good", "great"),
dissatisfied.terms = c("terrible", "poor", "rubbish")))
text<-c("The product was pretty good, I am satisfied",
"What a rubbish product",
"Terrible customer service but the product was good")
dfm_sentiment1 <- text %>% tokens() %>% dfm() %>% dfm_lookup(product.lexicon)
dfm_sentiment1
library(quanteda)
library(quanteda.dictionaries)
reviews_df <- data.frame(
User_ID = c(34, 17, 91, 2, 69, 88, 45, 23, 57, 77),
Review_Text = c("amazing product but the delivery was bad",
"good product but the packaging was awful",
"excellent quality but the size is terrible",
"nice design but the color is the worst",
"best product I have bought but the price disappointed me",
"amazing but bad delivery service",
"good but awful packaging",
"excellent but the color is terrible",
"nice but the price is the worst",
"best but the delivery time disappointed me"),
Product_ID = c(7, 22, 45, 8, 30, 11, 39, 46, 27, 33),
Review_Rating = c(3, 4, 2, 3, 5, 3, 4, 2, 3, 5)
)
corp_reviews <- corpus(reviews_df, text_field = "Review_Text")# Create corpus
output_lsd <- liwcalike(corp_reviews, data_dictionary_LSD2015)
output_lsd
head(output_mfd)
output_mfd <- quanteda.dictionaries::liwcalike(corp_hotel,
dictionary = data_dictionary_MFD)
head(output_mfd)
head(output_mfd)
View(output_lsd)
library(quanteda)
library(quanteda.dictionaries)
reviews_df <- data.frame(
User_ID = c(34, 17, 91, 2, 69, 88, 45, 23, 57, 77),
Review_Text = c("amazing product but the delivery was bad",
"good product but the packaging was awful",
"excellent quality but the size is terrible",
"nice design but the color is the worst",
"best product I have bought but the price disappointed me",
"amazing but bad delivery service",
"good but awful packaging",
"excellent but the color is terrible",
"nice but the price is the worst",
"best but the delivery time disappointed me"),
Product_ID = c(7, 22, 45, 8, 30, 11, 39, 46, 27, 33),
Review_Rating = c(3, 4, 2, 3, 5, 3, 4, 2, 3, 5)
)
corp_reviews <- corpus(reviews_df, text_field = "Review_Text")# Create corpus
output_lsd <- liwcalike(corp_reviews, data_dictionary_LSD2015)
View(output_lsd)
reviews_df <- data.frame(
User_ID = c(34, 17, 91, 2, 69, 88, 45, 23, 57, 77),
Review_Text = c("amazing product but the delivery was bad",
"good product but the packaging was awful",
"excellent quality but the size is terrible",
"nice design but the color is the worst",
"best product I have bought but the price disappointed me",
"amazing but bad delivery service",
"good but awful packaging",
"excellent but the color is terrible",
"nice but the price is the worst",
"best but the delivery time disappointed me"),
Product_ID = c(7, 22, 45, 8, 30, 11, 39, 46, 27, 33),
Review_Rating = c(3, 4, 2, 3, 5, 3, 4, 2, 3, 5)
)
corp_reviews <- corpus(reviews_df, text_field = "Review_Text")# Create corpus
library(sentimentr)
mytext2 <- corp_reviews %>% get_sentences() %>% sentiment()
mytext2
View(mytext2)
library(quanteda)
library(quanteda.dictionaries)
reviews_df <- data.frame(
User_ID = c(34, 17, 91, 2, 69, 88, 45, 23, 57, 77),
Review_Text = c("amazing product but the delivery was bad",
"good product but the packaging was awful",
"excellent quality but the size is terrible",
"nice design but the color is the worst",
"best product I have bought but the price disappointed me",
"amazing but bad delivery service",
"good but awful packaging",
"excellent but the color is terrible",
"nice but the price is the worst",
"best but the delivery time disappointed me"),
Product_ID = c(7, 22, 45, 8, 30, 11, 39, 46, 27, 33),
Review_Rating = c(3, 4, 2, 3, 5, 3, 4, 2, 3, 5)
)
library(magrittr)
library(dplyr)
set.seed(2)
reviews_df %>%
filter(User_ID %in% sample(unique(User_ID), 3)) %>%
mutate(review = get_sentences(Review_Text)) %$%
sentiment_by(review, User_ID) %>%
highlight()
View(reviews_df)
suppressMessages(suppressWarnings(library(tidyverse)))
suppressMessages(suppressWarnings(library(cleanNLP)))
cnlp_init_udpipe() # Loading namespace: udpipe which serves backend to cleanNLP
postag<-reviews_df %>%
cnlp_annotate(text= "Review_Text") # Outcome is list of tokens and documents
head(postag$token,n=10)
View(postag)
postag$token %>%
filter(xpos == "NN") %>% # you can play around with different POS
group_by(lemma) %>%
summarize(count = n())%>%
arrange(desc(count))
require(quanteda.textmodels)
require(caret)
set.seed(300)
id_train <- sample(1:10, 6, replace = FALSE)
# create docvar with ID
corp_reviews$id_numeric <- 1:ndoc(corp_reviews)
# get training set
dfmat_training <- corpus_subset(corp_reviews, id_numeric %in% id_train) %>%
tokens() %>% dfm() %>%dfm_remove(stopwords("english")) %>%dfm_wordstem()
dfmat_training<-dfm_weight(dfmat_training, scheme = "boolean")
# get test set (documents not in id_train)
dfmat_test <- corpus_subset(corp_reviews, !id_numeric %in% id_train) %>%
tokens() %>% dfm() %>%dfm_remove(stopwords("english")) %>%dfm_wordstem()
dfmat_test<-dfm_weight(dfmat_test, scheme = "boolean")
library(quanteda.textmodels)
tmod_nb <- textmodel_nb(dfmat_training, dfmat_training$Review_Rating)
summary(tmod_nb)
dfmat_matched <- dfm_match(dfmat_test, features = featnames(dfmat_training))
actual_class <- dfmat_matched$Review_Rating
predicted_class <- predict(tmod_nb, newdata = dfmat_matched)
tab_class <- table(actual_class, predicted_class)
tab_class
set.seed(145)
id_train <- sample(1:10, 6, replace = FALSE)
# create docvar with ID
corp_reviews$id_numeric <- 1:ndoc(corp_reviews)
# get training set
dfmat_training <- corpus_subset(corp_reviews, id_numeric %in% id_train) %>%
tokens() %>% dfm() %>%dfm_remove(stopwords("english")) %>%dfm_wordstem()
dfmat_training<-dfm_weight(dfmat_training, scheme = "boolean")
# get test set (documents not in id_train)
dfmat_test <- corpus_subset(corp_reviews, !id_numeric %in% id_train) %>%
tokens() %>% dfm() %>%dfm_remove(stopwords("english")) %>%dfm_wordstem()
dfmat_test<-dfm_weight(dfmat_test, scheme = "boolean")
library(quanteda.textmodels)
tmod_nb <- textmodel_nb(dfmat_training, dfmat_training$Review_Rating)
summary(tmod_nb)
dfmat_matched <- dfm_match(dfmat_test, features = featnames(dfmat_training))
actual_class <- dfmat_matched$Review_Rating
predicted_class <- predict(tmod_nb, newdata = dfmat_matched)
tab_class <- table(actual_class, predicted_class)
tab_class
data_df <- data.frame(age, gender, income, education, price, rating_and_reviews)
# Load required library
library(dplyr)
# Set the seed to make the example reproducible
set.seed(123)
# Create a vector of random ages between 18 and 60
age <- sample(18:60, 20, replace = TRUE)
# Create a vector of genders
gender <- sample(c("Male", "Female"), 20, replace = TRUE)
# Create a vector of random incomes between $20,000 and $100,000
income <- sample(20000:100000, 20, replace = TRUE)
# Create a vector of education levels
education <- sample(c("High School", "Bachelor's", "Master's", "PhD"), 20, replace = TRUE)
# Create a vector of random prices between $20 and $30
price <- round(runif(20, min = 20, max = 30),1)
# Create a vector of ratings and corresponding reviews
rating_and_reviews <- data.frame(
rating = c(5, 4, 3, 2, 1, 5, 4, 3, 2, 1, 5, 4, 3, 2, 1, 5, 4, 3, 2, 1),
reviews = c("I loved the toaster, it was amazing!",
"The toaster was good, but I've used better.",
"The toaster was okay, but not great.",
"I was disappointed with the toaster, it didn't meet my expectations.",
"I did not like the toaster, it was poor quality.",
"The toaster was excellent, I use it every day!",
"The toaster was useful, but not worth the price.",
"The toaster was not what I expected, I wouldn't buy it again.",
"I was not satisfied with the toaster, it didn't work as advertised.",
"I didn't like the toaster, it was too complicated to use.",
"The toaster was fantastic, it exceeded my expectations.",
"The toaster was good, but it was not worth the price.",
"I didn't enjoy the toaster, it was not useful to me.",
"I was unhappy with the toaster, it arrived damaged.",
"The toaster was terrible, I will not buy it again.",
"The toaster was wonderful, I will definitely buy it again!",
"The toaster was decent, but I've seen better.",
"The toaster was below average, it lacked key features.",
"I was frustrated with the toaster, it was not user-friendly.",
"The toaster was the worst I've used, I do not recommend it.")
)
# Sample rows from rating_and_reviews to randomize the order
rating_and_reviews <- sample_n(rating_and_reviews, 20, replace = FALSE)
data_df <- data.frame(age, gender, income, education, price, rating_and_reviews)
data_df <- data.frame(age, gender, income, education, price, rating_and_reviews)
# Regression Model 1
model1 <- lm(rating ~ price, data = data_df)
summary(model1)
# Regression Model 2
model2 <- lm(rating ~ age+ gender +income+ education+price, data = data_df)
summary(model2)
library(sentimentr)
data_df <- data_df %>%
get_sentences('reviews') %>%
sentiment()
# Regression Model 3
model3 <- lm(rating ~ age+ gender +income+ education+price+word_count+sentiment, data = data_df)
summary(model3)
source("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 3/Assignment 5.R")
