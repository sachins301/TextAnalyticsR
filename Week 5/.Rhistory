library(tidyverse)
library(quanteda)
library(tidytext)
library(topicmodels)
library(stm)
hotel_raw <- read_csv("../hotel-reviews.csv")
set.seed(1234)
hotel_raw<-hotel_raw[sample(nrow(hotel_raw), 1000), ]# take a small sample
knitr::opts_chunk$set(echo = TRUE, warning=FALSE, message=FALSE, fig.dim = c(3.5, 3.5))
options(width = 40)
# Load necessary libraries
library(tibble)
library(quanteda)
# Define product categories
product_categories <- c("laptop", "smartphone", "headphones", "smartwatch", "camera")
# Define some example reviews for each product category
laptop_reviews <- c("The battery life is great", "It's super fast",
"The screen is amazing")
smartphone_reviews <- c("The camera is superb", "Love the sleek design",
"It's very user friendly")
headphones_reviews <- c("The sound quality is excellent", "They are very comfortable",
"Too much bass")
smartwatch_reviews <- c("Tracks my workouts accurately", "Love the design",
"The battery life is excellent")
camera_reviews <- c("Takes high quality pictures", "It's very easy to use",
"Love the zoom feature")
# Create a tibble (similar to a data frame) with the reviews and product categories
reviews <- tibble(
review = c(laptop_reviews, smartphone_reviews, headphones_reviews,
smartwatch_reviews, camera_reviews),
product_category = rep(product_categories, each = 3)
)
corpus <- corpus(reviews, text_field = "review") # create a corpus from the review text
tokens <- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
tokens <- tokens_tolower(tokens) # convert all text to lowercase
tokens <- tokens_remove(tokens, stopwords("en")) # remove English stopwords
# Create a Document-Feature Matrix
dfm <- dfm(tokens)
library(seededlda)
# Train a topic model with 5 topics
lda <- seededlda::textmodel_lda(dfm, k = 5)
# Display the top words in each topic
knitr::kable(seededlda::terms(lda))
# You can also predict the topics of documents using topics()
dat <- quanteda::docvars(lda$data)
dat$topic <- seededlda::topics(lda)
knitr::kable(head(dat, 10))
library(readr)
# Create synthetic reviews for two hypothetical competitors
set.seed(12345)
product_types <- c("Smartwatch1", "Smartwatch2")
competitors <- c("Competitor1", "Competitor2")
themes <- list(
"battery life" = c("Great battery life!", "The battery drains too quickly.",
"Battery lasts all day.", "Battery life is not as advertised."),
"display quality" = c("The display is vibrant and sharp.", "Display is mediocre.",
"Love the OLED display!", "Screen resolution is subpar."),
"performance" = c("Super fast and responsive!", "Performance is laggy.",
"Handles all apps and games smoothly.", "Tends to freeze and crash."),
"price" = c("Excellent value for the price.", "Overpriced for what it offers.",
"Affordable and well worth it.", "Too expensive."),
"customer service" = c("Customer service was helpful and prompt.",
"Had a bad experience with customer service.",
"Customer support is top notch.",
"Customer service could be improved.")
)
# Generate synthetic dataset
reviews <- data.frame(
product = sample(product_types, 500, replace = TRUE),
competitor = sample(competitors, 500, replace = TRUE),
review = replicate(500, paste(sample(unlist(themes), 5), collapse = " "))
)
# Preprocessing steps
corpus <- corpus(reviews$review) # create a corpus from the review text
tokens <- tokens(corpus, remove_punct = TRUE, remove_numbers = TRUE, remove_symbols = TRUE)
tokens <- tokens_tolower(tokens) # convert all text to lowercase
tokens <- tokens_remove(tokens, stopwords("en")) # remove English stopwords
# Create a Document-Feature Matrix
dfm <- dfm(tokens)
# Train a topic model with 5 topics
model <- seededlda::textmodel_lda(dfm, k = 5)
# Extract the top terms for each topic
top_terms <- seededlda::terms(model)
# Print the top terms for each topic
print(top_terms)
# Assign each document to a topic
doc_topics <- model$theta
doc_max_topics <- apply(doc_topics, 1, which.max)
# Add the dominant topic to each review in the dataset
reviews$dominant_topic <- doc_max_topics
# Now, you can analyze the dominant topics for each competitor
table(reviews$competitor, reviews$dominant_topic)
# Now, you can analyze the dominant topics for each competitor
topic_distribution <- table(reviews$competitor, reviews$dominant_topic)
# Provide meaningful names to the rows and columns
rownames(topic_distribution) <- paste("Competitor", rownames(topic_distribution))
colnames(topic_distribution) <- paste("Topic", colnames(topic_distribution))
# Add margins (totals for each row and column)
topic_distribution <- addmargins(topic_distribution)
# Print the labeled table
print(topic_distribution)
library(tidyverse)
library(quanteda)
library(tidytext)
library(topicmodels)
library(stm)
hotel_raw <- read_csv("../hotel-reviews.csv")
set.seed(1234)
hotel_raw<-hotel_raw[sample(nrow(hotel_raw), 1000), ]# take a small sample
hotel_raw_token2 <- tokens(hotel_raw$Description, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE)
#Create DTM, but remove terms which occur in less than 1% of all documents
# and more than 90%
hotel_raw_dfm<-hotel_raw_token2 %>%
tokens_remove(stopwords(source = "smart")) %>%
#tokens_wordstem() %>%
tokens_tolower() %>%
dfm()%>%
dfm_trim(min_docfreq = 0.01, max_docfreq = 0.90, docfreq_type = "prop")
#hotel_raw_dfm[1:5,1:5]
hotel_demo_token1<-as.matrix(hotel_raw_dfm)
# dim(hotel_demo_token1)
hotel_demo_token1[1:3,1:8]
library(topicmodels)
set.seed(12345)
K <- 15
hotel_lda <-LDA(hotel_raw_dfm, K, method="Gibbs", control=list(iter = 200, verbose = 25))
term_topics <- tidy(hotel_lda, matrix = "beta")#Topic Term Probabilities # we tidy it up to use tidytext package
term_topics
top_terms <- term_topics %>%
group_by(topic) %>%
slice_max(beta, n = 5) %>%
arrange(topic, -beta)
top_terms
library(ggplot2)
top_terms %>%
mutate(term = reorder(term, beta)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free")
hotel_documents <- tidy(hotel_lda, matrix = "gamma")
hotel_documents
top_documents <- hotel_documents %>%
group_by(topic) %>%
slice_max(gamma, n = 5) %>%
ungroup() %>%
arrange(topic, -gamma)
top_documents
tidy(hotel_raw_dfm) %>% # taking the original dfm
filter(document == 'text6') %>%
arrange(desc(count))
assignments <- augment(hotel_lda)
assignments
library(ldatuning)
# Compute the K value "scores" from K=2 to K=25
result <- FindTopicsNumber(
hotel_raw_dfm,
topics = seq(from = 2, to = 25, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "VEM",#Gibbs can used too
control = list(seed = 1971),
mc.cores = 2L,
return_models= TRUE,
verbose = TRUE
)
# Plot the scores, with graphs labeled "minimize" or
# maximize based on whether it's a metric you want
# to minimize or maximize
FindTopicsNumber_plot(result)
ldaResult <- posterior(hotel_lda)# have a look a some of the results (posterior distributions)
attributes(ldaResult)
library(text2vec)
text2vec::perplexity(hotel_raw_dfm, ldaResult$terms, ldaResult$topics)
library(textmineR)
set.seed(12345)
model <- FitLdaModel(dtm = hotel_raw_dfm,
k = 20,
iterations = 200, # I usually recommend at least 500 iterations or more
burnin = 180,
alpha = 0.1,
beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE,
cpus = 2)# allows two cores for processing
# R-squared
# - only works for probabilistic models like LDA and CTM
model$r2
# log Likelihood (does not consider the prior)
plot(model$log_likelihood, type = "l")
summary(model$coherence)
hist(model$coherence,
col= "blue",
main = "Histogram of probabilistic coherence")
