rmse_elastic_net
library(tidyverse)
hotel_review<-read_csv("Data/hotel-reviews.csv")
library(tidyverse)
hotel_review<-read_csv("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 7/hotel-reviews.csv")
hotel_review<-read_csv("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 1/hotel-reviews.csv")
hotel_review<-sample_n(hotel_review, 5000)
# Convert binary variable, Is_Response to a continuous ratings variable
convert_fn <- function(x) {
sapply(x, function(y) {
if (y == "not happy") {
return(sample(1:2, size = 1, prob = c(0.5, 0.5)))
} else { # y == "happy"
return(sample(3:5, size = 1, prob = c(0.33, 0.33, 0.34)))
}
})
}
hotel_review$ratings <- convert_fn(hotel_review$Is_Response)
head(hotel_review)
library(caret)
library(magrittr)
# Set the seed for reproducibility
set.seed(123)
# Define the proportion of data you want to keep for training
trainIndex <- createDataPartition(hotel_review$ratings, p = 0.70, list = FALSE, times = 1)
# Create the training set
train_set <- hotel_review[trainIndex,]
# Create the testing set
test_set <- hotel_review[-trainIndex,]
library(sentimentr)
t1 <- train_set %>%
mutate(review = get_sentences(Description)) %$%
sentiment_by(review, User_ID)
train_set1 <- train_set %>% left_join(t1,by = "User_ID")
Train <- train_set1 %>% select(User_ID,ratings, word_count, ave_sentiment)
ctrl <- trainControl(method="cv", number=10)
# Create a linear regression model
model <- train(ratings ~ word_count+ ave_sentiment, data=Train, method="lm", trControl=ctrl)
# Print model results
print(model)
summary(model)
t2 <- test_set %>%
mutate(review = get_sentences(Description)) %$%
sentiment_by(review, User_ID)
test_set1 <- test_set %>% left_join(t2,by = "User_ID")
Test <- test_set1 %>% select(User_ID,ratings, word_count, ave_sentiment)
# Generate predictions on test data
predictions <- predict(model, newdata = Test)
# Calculate R-squared and RMSE
rsq_rmse <- postResample(pred = predictions, obs = Test$ratings)
# Print the results
print(rsq_rmse)
library(caret)
library(quanteda)
review_corpus<-corpus(hotel_review, text_field = "Description")
hotel_review_token <- tokens(review_corpus, what = "word",
remove_numbers = TRUE, remove_punct = TRUE,
remove_symbols = TRUE)
#Create DFM
hotel_review_tfidf<-hotel_review_token %>%
tokens_remove(stopwords(source = "smart")) %>%
tokens_wordstem() %>%
tokens_tolower() %>%
dfm() %>%
dfm_trim( min_termfreq = 50, min_docfreq = 10) %>%
dfm_tfidf()
library("quanteda.textmodels")
mylsa <- textmodel_lsa(hotel_review_tfidf, nd = 100)
new.dataset <- data.frame(mylsa$docs)
new.dataset$y.var <- hotel_review$ratings
test_index <- createDataPartition(new.dataset$y.var, p = .2, list = F)
Test_LSA<-new.dataset[test_index,]
Train_LSA<-new.dataset[-test_index,]
ctrl <- trainControl(method="cv", number=10)
#Training data
# Create a linear regression model
model <- train(y.var ~ ., data=Train_LSA, method="lm", trControl=ctrl)
summary(model)
# Generate predictions on test data
predictions1 <- predict(model, newdata = Test_LSA)
# Calculate R-squared and RMSE
rsq_rmse1 <- postResample(pred = predictions1, obs = Test_LSA$y.var)
# Print the results
print(rsq_rmse1)
hotel_review_tfidf1 <- data.frame(y.var=hotel_review$ratings,hotel_review_tfidf )
hotel_review_tfidf1 <- hotel_review_tfidf1 %>% select(-doc_id)
test_index <- createDataPartition(hotel_review_tfidf1$y.var, p = .2, list = F)
hotel_review_tfidf1 <- data.frame(y.var=hotel_review$ratings,hotel_review_tfidf )
hotel_review_tfidf1 <- hotel_review_tfidf1 %>% select(-doc_id)
hotel_review_tfidf1 <- data.frame(y.var=hotel_review$ratings,hotel_review_tfidf )
hotel_review_tfidf1 <- hotel_review_tfidf1 %>% select(-doc_id)
# Convert the dfm object to a data.frame
hotel_review_tfidf_df <- convert(hotel_review_tfidf, to = "data.frame")
# Combine the ratings with the tfidf data frame
hotel_review_tfidf1 <- data.frame(y.var = hotel_review$ratings, hotel_review_tfidf_df)
test_index <- createDataPartition(hotel_review_tfidf1$y.var, p = .2, list = F)
test_text <- hotel_review_tfidf1[test_index, ]
train_text <- hotel_review_tfidf1[-test_index, ]
## Set up repeated k-fold cross-validation
train_control <- trainControl(method = "cv", number = 2)
# Fit the SVR model
set.seed(12345)
SVM_Linear <- train(
y.var ~., data = train_text, method = "svmLinear",
trControl = train_control,
preProcess = c("center","scale"),
tuneLength = 7
)
# Predict on the test set
predictions <- predict(SVM_Linear, newdata = test_text)
new_data <- dfm_match(df_test, features = featnames(df_training))
# Predict with the final Ridge model
predictions_ridge <- predict(final_ridge, newx = new_data)
# Predict with the final Lasso model
predictions_lasso <- predict(final_lasso, newx = new_data)
# Predict with the final Elastic Net model
predictions_elastic_net <- predict(final_elastic_net, newx = new_data)
library(caret)
# Calculate R-squared for the Ridge model
r_squared_ridge <- postResample(pred = predictions_ridge, obs = hotel_test$ratings)[2]
r_squared_ridge
# Calculate RMSE for the Ridge model
rmse_ridge <- postResample(pred = predictions_ridge, obs = hotel_test$ratings)[1]
rmse_ridge
# Calculate R-squared for the Lasso model
r_squared_lasso <- postResample(pred = predictions_lasso, obs = hotel_test$ratings)[2]
r_squared_lasso
# Calculate RMSE for the Lasso model
rmse_lasso <- postResample(pred = predictions_lasso, obs = hotel_test$ratings)[1]
rmse_lasso
# Calculate R-squared for the Elastic Net model
r_squared_elastic_net <- postResample(pred = predictions_elastic_net, obs = hotel_test$ratings)[2]
r_squared_elastic_net
# Calculate RMSE for the Elastic Net model
rmse_elastic_net <- postResample(pred = predictions_elastic_net, obs = hotel_test$ratings)[1]
rmse_elastic_net
library(tidyverse)
library(bigrquery)
install.packages("gganimate")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("sf")
install.packages("gganimate")
install.packages("ggplot2")
install.packages("dplyr")
library(gganimate)
library(dplyr)
library(ggplot2)
library(sf)
project_id <- 'is-6850-090-advanced-sql'
setwd(getwd())
install.packages("ggplot2")
install.packages("dplyr")
library(tidyverse)
library(bigrquery)
install.packages("gganimate")
install.packages("dplyr")
install.packages("ggplot2")
install.packages("sf")
install.packages("dplyr")
install.packages("ggplot2")
library(gganimate)
library(dplyr)
library(ggplot2)
library(sf)
project_id <- 'is-6850-090-advanced-sql'
setwd(getwd())
install.packages("ggplot2")
library(tidyverse)
library(bigrquery)
library(gganimate)
library(dplyr)
library(ggplot2)
library(sf)
project_id <- 'is-6850-090-advanced-sql'
setwd(getwd())
get_query <- function(x){
con <- file(x, "r")
contents <- readChar(con, file.info(x)$size)
close(con)
return(contents)
}
sql4 <- get_query('final_project3.sql')
qq4 <- bq_project_query(project_id, sql4)
sql4 <- get_query('final_project3.sql')
qq4 <- bq_project_query(project_id, sql4)
dta4 <- bq_table_download(qq4)
dta4 <- dta4 %>%
mutate(hour = factor(hour, levels = 0:23))
# If you have a shapefile of the stations, load it
# stations_shapefile <- st_read("path_to_shapefile.shp")
# Create the animated plot
p <- ggplot(dta4, aes(x = lon, y = lat, size = bike_count, color = bike_count)) +
geom_point(alpha = 0.7) +
scale_size(range = c(3, 10), name = "Bike Count") +
scale_color_viridis_c() +
labs(title = "Number of Bikes by Hour: {closest_state}", x = "Longitude", y = "Latitude") +
theme_minimal() +
transition_states(hour, transition_length = 2, state_length = 1) +
ease_aes('linear')
# Animate the plot
animate(p, nframes = 24, fps = 2, width = 800, height = 600, renderer = gifski_renderer("animated_bikes.gif"))
# Animate the plot
animate(p, nframes = 24, fps = 2, width = 800, height = 600, renderer = gifski_renderer("animated_bikes.gif"))
library(wordVectors)
library(magrittr)
if (!require(wordVectors)) {
if (!(require(devtools))) {
install.packages("devtools")
}
devtools::install_github("bmschmidt/wordVectors")
}
# Assuming the binary file "cookbook_vectors.bin" has already been created
model = read.vectors("cookbook_vectors.bin")
library(wordVectors)
library(magrittr)
# Assuming the binary file "cookbook_vectors.bin" has already been created
model = read.vectors("cookbook_vectors.bin")
# Confirm the working directory
print(getwd())
setwd("C:/Users/sachi/OneDrive - University of Utah/Sem3/MKTG6640 TA/Week 9")
# Confirm the working directory
print(getwd())
# Assuming the binary file "cookbook_vectors.bin" has already been created
model = read.vectors("cookbook_vectors.bin")
# Print the results
print(closest_words)
# Assuming the binary file "cookbook_vectors.bin" has already been created
model = read.vectors("cookbook_vectors.bin")
# Find the closest word to "apricot"
closest_words <- model %>% closest_to("apricot")
# Print the results
print(closest_words)
some_fruit = closest_to(model,model[[c("fruit","peach","cherry","nectarine")]],10)
fruity = model[[some_fruit$word,average=F]]
plot(fruity,method="pca")
# Find the closest word to "apricot"
closest_words <- model %>% closest_to("plum")
# Print the results
print(closest_words)
# Find the closest word to "apricot"
closest_words <- model %>% closest_to("peach")
# Print the results
print(closest_words)
set.seed(10)
centers = 50
clustering = kmeans(model,centers=centers,iter.max = 40)
sapply(sample(1:centers,10),function(n) {
names(clustering$cluster[clustering$cluster==n][1:10])
})
set.seed(10)
centers = 50
clustering = kmeans(model,centers=centers,iter.max = 40)
ingredients = c("madeira","chicken","saucepan","carrots")
term_set = lapply(ingredients,
function(ingredient) {
nearest_words = model %>% closest_to(model[[ingredient]],20)
nearest_words$word
}) %>% unlist
subset = model[[term_set,average=F]]
subset %>% closest_to("saucepan")
tastes = model[[c("sweet","sour"),average=F]]
# model[1:3000,] here restricts to the 3000 most common words in the set.
sweet_and_sourness = model[1:3000,] %>% cosineSimilarity(tastes)
# Filter to the top 10 sweet or sour.
sweet_and_sourness = sweet_and_sourness[
rank(-sweet_and_sourness[,1])<10 |
rank(-sweet_and_sourness[,2])<10,
]
plot(sweet_and_sourness,type='n')
text(sweet_and_sourness,labels=rownames(sweet_and_sourness))
# Load necessary libraries
library(wordVectors)
library(dplyr)
library(text2vec)
library(ggplot2)
# Load your pre-trained word vectors
model = read.vectors("Data/IMDB_vectors.bin")
# Example movie data with real-life movie names and genres
movie_data <- data.frame(
title = c("Inception", "The Dark Knight", "Forrest Gump", "Pulp Fiction", "The Matrix",
"Titanic", "The Avengers", "Get Out", "La La Land", "Interstellar", "Gladiator",
"Schindler's List", "The Godfather", "The Shawshank Redemption", "The Lion King",
"Jurassic Park", "The Silence of the Lambs", "Star Wars", "Back to the Future",
"The Terminator", "Goodfellas", "Braveheart", "The Sixth Sense", "Avatar",
"Fight Club", "The Departed", "The Prestige", "The Social Network",
"Mad Max: Fury Road",
"Guardians of the Galaxy"),
genres = c("sci-fi, thriller",
"action, thriller",
"drama, romance",
"crime, drama",
"sci-fi, action",
"drama, romance",
"action, sci-fi",
"horror, thriller",
"romance, musical",
"sci-fi, drama",
"action, drama",
"drama, history",
"crime, drama",
"drama, crime",
"animation, adventure",
"sci-fi, adventure",
"crime, thriller",
"sci-fi, adventure",
"sci-fi, comedy",
"sci-fi, action",
"crime, drama",
"action, drama",
"horror, mystery",
"sci-fi, adventure",
"drama, thriller",
"crime, thriller",
"drama, mystery",
"drama, biography",
"action, adventure",
"action, sci-fi")
)
# Function to get embeddings for a list of genres
get_genre_embedding <- function(genres, model) {
genre_list <- unlist(strsplit(genres, ",\\s*"))
genre_list <- tolower(genre_list)
genre_list <- genre_list[genre_list %in% rownames(model)]
if (length(genre_list) == 0) return(NULL)
colMeans(model[[genre_list, average = FALSE]])
}
# Calculate embeddings for each movie based on genres
movie_data$embedding <- lapply(movie_data$genres, get_genre_embedding, model)
# Filter out movies with no valid embeddings
movie_data <- movie_data[!sapply(movie_data$embedding, is.null), ]
embeddings <- do.call(rbind, movie_data$embedding)
# Function to calculate cosine similarity using text2vec
calculate_cosine_similarity <- function(a, b) {
return(text2vec::sim2(a, b, method = "cosine", norm = "l2"))
}
# Recommendation function
recommend_movies <- function(movie_title, movie_data, top_n = 3) {
# Find the embedding of the input movie
movie_index <- which(movie_data$title == movie_title)
if (length(movie_index) == 0) {
stop("Movie not found in the dataset.")
}
movie_embedding <- matrix(movie_data$embedding[[movie_index]], nrow = 1)
# Calculate cosine similarities
similarities <- calculate_cosine_similarity(embeddings, movie_embedding)[, 1]
# Exclude the input movie from recommendations
similarities[movie_index] <- -Inf
# Get the indices of the top_n most similar movies
top_indices <- order(similarities, decreasing = TRUE)[1:top_n]
# Return the titles and genres of the recommended movies
recommendations <- movie_data[top_indices, c("title", "genres")]
return(recommendations)
}
# Example: Recommend movies similar to "The Terminator"
recommendations <- recommend_movies("The Terminator", movie_data)
print(recommendations)
library(wordVectors)
library(dplyr)
model = read.vectors("Data/IMDB_vectors.bin")
model %>% closest_to("connery")# since the embeddings are in lower case,
model %>% closest_to(~"batman"+"villain")
model = read.vectors("Data/IMDB_vectors.bin")
